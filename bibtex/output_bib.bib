@INPROCEEDINGS{10.1145/3295750.3298946,
  AUTHOR =       {Qu, Chen and Yang, Liu and Croft, W. Bruce and
                  Scholer, Falk and Zhang, Yongfeng},
  TITLE =        {answer interaction in non-factoid question answering
                  systems},
  YEAR =         {2019},
  BOOKTITLE =    {Proceedings of the 2019 Conference on Human
                  Information Interaction and Retrieval},
  SERIES =       {CHIIR '19},
  PAGES =        {249–253},
  ABSTRACT =     {Information retrieval systems are evolving from
                  document retrieval to answer retrieval. Web search
                  logs provide large amounts of data about how people
                  interact with ranked lists of documents, but very
                  little is known about interaction with answer texts.
                  In this paper, we use Amazon Mechanical Turk to
                  investigate three answer presentation and
                  interaction approaches in a non-factoid question
                  answering setting. We find that people perceive and
                  react to good and bad answers very differently, and
                  can identify good answers relatively quickly. Our
                  results provide the basis for further investigation
                  of effective answer interaction and feedback
                  methods.}
  NUMPAGES =     {5}
  LOCATION =     {Glasgow, Scotland UK}
  KEYWORDS =     {information-seeking, answer interaction, non-factoid
                  question answering, user interaction, answer
                  presentation}
  ADDRESS =      {New York, NY, USA}
  PUBLISHER =    {Association for Computing Machinery}
  ISBN =         {9781450360258}
  URL =          {https://doi.org/10.1145/3295750.3298946},
  DOI =          {10.1145/3295750.3298946},
}


@INPROCEEDINGS{10.1145/3343413.3378015,
  AUTHOR =       {Bauer, Christine},
  TITLE =        {multi-method evaluation: leveraging multiple methods
                  to answer what you were looking for},
  YEAR =         {2020},
  BOOKTITLE =    {Proceedings of the 2020 Conference on Human
                  Information Interaction and Retrieval},
  SERIES =       {CHIIR '20},
  PAGES =        {472–474},
  ABSTRACT =     {Research in the field of information retrieval and
                  recommendation mostly focuses on one single
                  evaluation method and one single quality objective.
                  On the one hand, many research endeavors focus on
                  system-centric evaluation from an algorithmic
                  perspective and consider the context of use only to
                  a minor extent. On the other hand, there are
                  research endeavors focusing on user-centric
                  approaches to the design and evaluation of systems.
                  However, algorithmic quality and perceived quality
                  of user experience do not necessarily match. Thus,
                  it is essential for system evaluation to
                  substantially integrate multiple evaluation methods
                  that cover a variety of relevant aspects and
                  perspectives. Only such an integrated combination of
                  methods may lead to a deep understanding of users,
                  their behavior, and experience in their interaction
                  with a system. This half-day tutorial follows the
                  objective to raise awareness in the CHIIR community
                  concerning the significance of using multiple
                  methods in the evaluation of information retrieval
                  and recommender systems. The tutorial illustrates
                  the "blind spots'' when using single methods. It
                  introduces the concept of "multi-method evaluation''
                  and discusses its benefits and challenges. While
                  multi-method evaluations may be designed very
                  flexibly, the tutorial presents broadly-defined
                  basic options of how multiple methods may be
                  integrated in an evaluation design. In group work,
                  participants are encouraged to select and fine-tune
                  a specific design that best matches their research
                  endeavor's purpose.}
  NUMPAGES =     {3}
  LOCATION =     {Vancouver BC, Canada}
  KEYWORDS =     {multi-methods, evaluation, information retrieval,
                  context of use, recommender systems}
  ADDRESS =      {New York, NY, USA}
  PUBLISHER =    {Association for Computing Machinery}
  ISBN =         {9781450368926}
  URL =          {https://doi.org/10.1145/3343413.3378015},
  DOI =          {10.1145/3343413.3378015},
}


@INPROCEEDINGS{10.1145/3406522.3446027,
  AUTHOR =       {Cambazoglu, B. Barla and Tavakoli, Leila and
                  Scholer, Falk and Sanderson, Mark and Croft, Bruce},
  TITLE =        {an intent taxonomy for questions asked in web
                  search},
  YEAR =         {2021},
  BOOKTITLE =    {Proceedings of the 2021 Conference on Human
                  Information Interaction and Retrieval},
  SERIES =       {CHIIR '21},
  PAGES =        {85–94},
  ABSTRACT =     {We present a new, multi-faceted taxonomy to classify
                  questions asked in web search engines based on the
                  question intent, types of entities mentioned, types
                  of question words, and granularity of the expected
                  answer. Built based on the inspection of 1,000 real-
                  life questions issued to a web search engine, the
                  taxonomy reflects the recent search behavior of
                  users and enables deep understanding of user
                  intents, goals, and expected answers. This taxonomy
                  is more fine-grained than previous query taxonomies,
                  and is designed with the ultimate goal of reducing
                  the inherent ambiguity in determining the intent of
                  questions. In addition, we describe the formal
                  procedure for conducting an editorial study of the
                  taxonomy including its evaluation. The adopted
                  procedure aims to increase assessor agreement
                  without incurring too much overhead. Our results
                  demonstrate that, despite being more fine-grained,
                  the proposed intent categories result in higher
                  agreement between assessors compared to an existing,
                  commonly used taxonomy.}
  NUMPAGES =     {10}
  LOCATION =     {Canberra ACT, Australia}
  KEYWORDS =     {web search engines, question intent taxonomy,
                  editorial study}
  ADDRESS =      {New York, NY, USA}
  PUBLISHER =    {Association for Computing Machinery}
  ISBN =         {9781450380553}
  URL =          {https://doi.org/10.1145/3406522.3446027},
  DOI =          {10.1145/3406522.3446027},
}


@INPROCEEDINGS{10.1145/3498366.3505815,
  AUTHOR =       {Thomas, Paul and Kazai, Gabriella and White, Ryen
                  and Craswell, Nick},
  TITLE =        {the crowd is made of people: observations from
                  large-scale crowd labelling},
  YEAR =         {2022},
  BOOKTITLE =    {ACM SIGIR Conference on Human Information
                  Interaction and Retrieval},
  SERIES =       {CHIIR '22},
  PAGES =        {25–35},
  ABSTRACT =     {Like many other researchers, at Microsoft Bing we
                  use external “crowd” judges to label results from a
                  search engine—especially, although not exclusively,
                  to obtain relevance labels for offline evaluation in
                  the Cranfield tradition. Crowdsourced labels are
                  relatively cheap, and hence very popular, but are
                  prone to disagreements, spam, and various biases
                  which appear to be unexplained “noise” or “error”.
                  In this paper, we provide examples of problems we
                  have encountered running crowd labelling at large
                  scale and around the globe, for search evaluation in
                  particular. We demonstrate effects due to the time
                  of day and day of week that a label is given;
                  fatigue; anchoring; exposure; left-side bias; task
                  switching; and simple disagreement between judges.
                  Rather than simple “error”, these effects are
                  consistent with well-known physiological and
                  cognitive factors. “The crowd” is not some abstract
                  machinery, but is made of people. Human factors that
                  affect people’s judgement behaviour must be
                  considered when designing research evaluations and
                  in interpreting evaluation metrics.}
  NUMPAGES =     {11}
  LOCATION =     {Regensburg, Germany}
  KEYWORDS =     {Crowdsourcing, Cognitive biases, Quality control}
  ADDRESS =      {New York, NY, USA}
  PUBLISHER =    {Association for Computing Machinery}
  ISBN =         {9781450391863}
  URL =          {https://doi.org/10.1145/3498366.3505815},
  DOI =          {10.1145/3498366.3505815},
}


@INPROCEEDINGS{10.1145/2911451.2914671,
  AUTHOR =       {Bailey, Peter and Moffat, Alistair and Scholer, Falk
                  and Thomas, Paul},
  TITLE =        {uqv100: a test collection with query variability},
  YEAR =         {2016},
  BOOKTITLE =    {Proceedings of the 39th International ACM SIGIR
                  Conference on Research and Development in
                  Information Retrieval},
  SERIES =       {SIGIR '16},
  PAGES =        {725–728},
  ABSTRACT =     {We describe the UQV100 test collection, designed to
                  incorporate variability from users. Information
                  need ?backstories? were written for 100 topics (or
                  sub-topics) from the TREC 2013 and 2014 Web Tracks.
                  Crowd workers were asked to read the backstories,
                  and provide the queries they would use; plus effort
                  estimates of how many useful documents they would
                  have to read to satisfy the need. A total of 10,835
                  queries were collected from 263 workers. After
                  normalization and spell-correction, 5,764 unique
                  variations remained; these were then used to
                  construct a document pool via Indri-BM25 over the
                  ClueWeb12-B corpus. Qualified crowd workers made
                  relevance judgments relative to the backstories,
                  using a relevance scale similar to the original TREC
                  approach; first to a pool depth of ten per query,
                  then deeper on a set of targeted documents. The
                  backstories, query variations, normalized and spell-
                  corrected queries, effort estimates, run outputs,
                  and relevance judgments are made available
                  collectively as the UQV100 test collection. We also
                  make available the judging guidelines and the gold
                  hits we used for crowd-worker qualification and spam
                  detection. We believe this test collection will
                  unlock new opportunities for novel investigations
                  and analysis, including for problems such as task-
                  intent retrieval performance and consistency
                  (independent of query variation), query clustering,
                  query difficulty prediction, and relevance feedback,
                  among others.}
  NUMPAGES =     {4}
  LOCATION =     {Pisa, Italy}
  KEYWORDS =     {backstory, clueweb, information retrieval, trec,
                  information need, test collection, user variability}
  ADDRESS =      {New York, NY, USA}
  PUBLISHER =    {Association for Computing Machinery}
  ISBN =         {9781450340694}
  URL =          {https://doi.org/10.1145/2911451.2914671},
  DOI =          {10.1145/2911451.2914671},
}


@INPROCEEDINGS{10.1145/2766462.2767703,
  AUTHOR =       {Eickhoff, Carsten and Dungs, Sebastian and Tran,
                  Vu},
  TITLE =        {an eye-tracking study of query reformulation},
  YEAR =         {2015},
  BOOKTITLE =    {Proceedings of the 38th International ACM SIGIR
                  Conference on Research and Development in
                  Information Retrieval},
  SERIES =       {SIGIR '15},
  PAGES =        {13–22},
  ABSTRACT =     {Information about a user's domain knowledge and
                  interest can be important signals for many
                  information retrieval tasks such as query suggestion
                  or result ranking. State-of-the-art user models rely
                  on coarse-grained representations of the user's
                  previous knowledge about a topic or domain. In this
                  paper, we study query refinement using eye-tracking
                  in order to gain precise and detailed insight into
                  which terms the user was exposed to in a search
                  session and which ones they showed a particular
                  interest in. We measure fixations on the term level,
                  allowing for a detailed model of user attention. To
                  allow for a wide-spread exploitation of our
                  findings, we generalize from the restrictive eye-
                  gaze tracking to using more accessible signals:
                  mouse cursor traces. Based on the public API of a
                  popular search engine, we demonstrate how query
                  suggestion candidates can be ranked according to
                  traces of user attention and interest, resulting in
                  significantly better performance than achieved by an
                  attention-oblivious industry solution. Our
                  experiments suggest that modelling term-level user
                  attention can be achieved with great reliability and
                  holds significant potential for supporting a range
                  of traditional IR tasks.}
  NUMPAGES =     {10}
  LOCATION =     {Santiago, Chile}
  KEYWORDS =     {query suggestion, query refinement, eye-gaze
                  tracking, mouse cursor tracking, query
                  reformulation, domain expertise, knowledge
                  acquisition}
  ADDRESS =      {New York, NY, USA}
  PUBLISHER =    {Association for Computing Machinery}
  ISBN =         {9781450336215}
  URL =          {https://doi.org/10.1145/2766462.2767703},
  DOI =          {10.1145/2766462.2767703},
}


@INPROCEEDINGS{10.1145/3331184.3331186,
  AUTHOR =       {Guo, Yangyang and Cheng, Zhiyong and Nie, Liqiang
                  and Liu, Yibing and Wang, Yinglong and Kankanhalli,
                  Mohan},
  TITLE =        {quantifying and alleviating the language prior
                  problem in visual question answering},
  YEAR =         {2019},
  BOOKTITLE =    {Proceedings of the 42nd International ACM SIGIR
                  Conference on Research and Development in
                  Information Retrieval},
  SERIES =       {SIGIR'19},
  PAGES =        {75–84},
  ABSTRACT =     {Benefiting from the advancement of computer vision,
                  natural language processing and information
                  retrieval techniques, visual question answering
                  (VQA), which aims to answer questions about an image
                  or a video, has received lots of attentions over the
                  past few years. Although some progress has been
                  achieved so far, several studies have pointed out
                  that current VQA models are heavily affected by the
                  language prior problem, which means they tend to
                  answer questions based on the co-occurrence patterns
                  of question keywords (e.g., how many) and answers
                  (e.g., 2) instead of understanding images and
                  questions. Existing methods attempt to solve this
                  problem by either balancing the biased datasets or
                  forcing models to better understand images. However,
                  only marginal effects and even performance
                  deterioration are observed for the first and second
                  solution, respectively. In addition, another
                  important issue is the lack of measurement to
                  quantitatively measure the extent of the language
                  prior effect, which severely hinders the advancement
                  of related techniques.In this paper, we make
                  contributions to solve the above problems from two
                  perspectives. Firstly, we design a metric to
                  quantitatively measure the language prior effect of
                  VQA models. The proposed metric has been
                  demonstrated to be effective in our empirical
                  studies. Secondly, we propose a regularization
                  method (i.e., score regularization module) to
                  enhance current VQA models by alleviating the
                  language prior problem as well as boosting the
                  backbone model performance. The proposed score
                  regularization module adopts a pair-wise learning
                  strategy, which makes the VQA models answer the
                  question based on the reasoning of the image (upon
                  this question) instead of basing on question-answer
                  patterns observed in the biased training set. The
                  score regularization module is flexible to be
                  integrated into various VQA models. We conducted
                  extensive experiments over two popular VQA datasets
                  (i.e., VQA 1.0 and VQA 2.0) and integrated the score
                  regularization module into three state-of-the-art
                  VQA models. Experimental results show that the score
                  regularization module can not only effectively
                  reduce the language prior problem of these VQA
                  models but also consistently improve their question
                  answering accuracy.}
  NUMPAGES =     {10}
  LOCATION =     {Paris, France}
  KEYWORDS =     {language prior problem, visual question answering,
                  evaluation metric}
  ADDRESS =      {New York, NY, USA}
  PUBLISHER =    {Association for Computing Machinery}
  ISBN =         {9781450361729}
  URL =          {https://doi.org/10.1145/3331184.3331186},
  DOI =          {10.1145/3331184.3331186},
}


@INPROCEEDINGS{10.1145/3397271.3401206,
  AUTHOR =       {Dalton, Jeffrey and Xiong, Chenyan and Kumar,
                  Vaibhav and Callan, Jamie},
  TITLE =        {cast-19: a dataset for conversational information
                  seeking},
  YEAR =         {2020},
  BOOKTITLE =    {Proceedings of the 43rd International ACM SIGIR
                  Conference on Research and Development in
                  Information Retrieval},
  SERIES =       {SIGIR '20},
  PAGES =        {1985–1988},
  ABSTRACT =     {CAsT-19 is a new dataset that supports research on
                  conversational information seeking. The corpus is
                  38,426,252 passages from the TREC Complex Answer
                  Retrieval (CAR) and Microsoft MAchine Reading
                  COmprehension (MARCO) datasets. Eighty information
                  seeking dialogues (30 train, 50 test) are an average
                  of 9 to 10 questions long. A dialogue may explore a
                  topic broadly or drill down into subtopics.
                  Questions contain ellipsis, implied context, mild
                  topic shifts, and other characteristics of human
                  conversation that may prevent them from being
                  understood in isolation. Relevance assessments are
                  provided for 30 training topics and 20 test
                  topics.CAsT-19 promotes research on conversational
                  information seeking by defining it as a task in
                  which effective passage selection requires
                  understanding a question's context (the dialogue
                  history). It focuses attention on user modeling,
                  analysis of prior retrieval results, transformation
                  of questions into effective queries, and other
                  topics that have been difficult to study with
                  existing datasets.}
  NUMPAGES =     {4}
  LOCATION =     {Virtual Event, China}
  KEYWORDS =     {dataset, conversational search, conversational
                  information seeking}
  ADDRESS =      {New York, NY, USA}
  PUBLISHER =    {Association for Computing Machinery}
  ISBN =         {9781450380164}
  URL =          {https://doi.org/10.1145/3397271.3401206},
  DOI =          {10.1145/3397271.3401206},
}


@INPROCEEDINGS{10.1145/2505515.2505682,
  AUTHOR =       {Hassan, Ahmed and Shi, Xiaolin and Craswell, Nick
                  and Ramsey, Bill},
  TITLE =        {beyond clicks: query reformulation as a predictor of
                  search satisfaction},
  YEAR =         {2013},
  BOOKTITLE =    {Proceedings of the 22nd ACM International Conference
                  on Information &amp; Knowledge Management},
  SERIES =       {CIKM '13},
  PAGES =        {2019–2028},
  ABSTRACT =     {To understand whether a user is satisfied with the
                  current search results, implicit behavior is a
                  useful data source, with clicks being the best-known
                  implicit signal. However, it is possible for a non-
                  clicking user to be satisfied and a clicking user to
                  be dissatisfied. Here we study additional implicit
                  signals based on the relationship between the user's
                  current query and the next query, such as their
                  textual similarity and the inter-query time. Using a
                  large unlabeled dataset, a labeled dataset of
                  queries and a labeled dataset of user tasks, we
                  analyze the relationship between these signals. We
                  identify an easily-implemented rule that indicates
                  dissatisfaction: that a similar query issued within
                  a time interval that is short enough (such as five
                  minutes) implies dissatisfaction. By incorporating
                  additional query-based features in the model, we
                  show that a query-based model (with no click
                  information) can indicate satisfaction more
                  accurately than click-based models. The best model
                  uses both query and click features. In addition, by
                  comparing query sequences in successful tasks and
                  unsuccessful tasks, we observe that search success
                  is an incremental process for successful tasks with
                  multiple queries.}
  NUMPAGES =     {10}
  LOCATION =     {San Francisco, California, USA}
  KEYWORDS =     {success prediction, search tasks, query
                  reformulation, re-querying behavior}
  ADDRESS =      {New York, NY, USA}
  PUBLISHER =    {Association for Computing Machinery}
  ISBN =         {9781450322638}
  URL =          {https://doi.org/10.1145/2505515.2505682},
  DOI =          {10.1145/2505515.2505682},
}


@INPROCEEDINGS{10.1145/1645953.1645966,
  AUTHOR =       {Huang, Jeff and Efthimiadis, Efthimis N.},
  TITLE =        {analyzing and evaluating query reformulation
                  strategies in web search logs},
  YEAR =         {2009},
  BOOKTITLE =    {Proceedings of the 18th ACM Conference on
                  Information and Knowledge Management},
  SERIES =       {CIKM '09},
  PAGES =        {77–86},
  ABSTRACT =     {Users frequently modify a previous search query in
                  hope of retrieving better results. These
                  modifications are called query reformulations or
                  query refinements. Existing research has studied how
                  web search engines can propose reformulations, but
                  has given less attention to how people perform query
                  reformulations. In this paper, we aim to better
                  understand how web searchers refine queries and form
                  a theoretical foundation for query reformulation. We
                  study users' reformulation strategies in the context
                  of the AOL query logs. We create a taxonomy of query
                  refinement strategies and build a high precision
                  rule-based classifier to detect each type of
                  reformulation. Effectiveness of reformulations is
                  measured using user click behavior. Most
                  reformulation strategies result in some benefit to
                  the user. Certain strategies like add/remove words,
                  word substitution, acronym expansion, and spelling
                  correction are more likely to cause clicks,
                  especially on higher ranked results. In contrast,
                  users often click the same result as their previous
                  query or select no results when forming acronyms and
                  reordering words. Perhaps the most surprising
                  finding is that some reformulations are better
                  suited to helping users when the current results are
                  already fruitful, while other reformulations are
                  more effective when the results are lacking. Our
                  findings inform the design of applications that can
                  assist searchers; examples are described in this
                  paper.}
  NUMPAGES =     {10}
  LOCATION =     {Hong Kong, China}
  KEYWORDS =     {query reformulation, query log analysis, search
                  effectiveness}
  ADDRESS =      {New York, NY, USA}
  PUBLISHER =    {Association for Computing Machinery}
  ISBN =         {9781605585123}
  URL =          {https://doi.org/10.1145/1645953.1645966},
  DOI =          {10.1145/1645953.1645966},
}


@INPROCEEDINGS{10.1145/3459637.3482231,
  AUTHOR =       {Aliannejadi, Mohammad and Azzopardi, Leif and
                  Zamani, Hamed and Kanoulas, Evangelos and Thomas,
                  Paul and Craswell, Nick},
  TITLE =        {analysing mixed initiatives and search strategies
                  during conversational search},
  YEAR =         {2021},
  BOOKTITLE =    {Proceedings of the 30th ACM International Conference
                  on Information &amp; Knowledge Management},
  SERIES =       {CIKM '21},
  PAGES =        {16–26},
  ABSTRACT =     {Information seeking conversations between users and
                  Conversational Search Agents (CSAs) consist of
                  multiple turns of interaction. While users initiate
                  a search session, ideally a CSA should sometimes
                  take the lead in the conversation by obtaining
                  feedback from the user by offering query suggestions
                  or asking for query clarifications i.e. mixed
                  initiative. This creates the potential for more
                  engaging conversational searches, but substantially
                  increases the complexity of modelling and evaluating
                  such scenarios due to the large interaction space
                  coupled with the trade-offs between the costs and
                  benefits of the different interactions. In this
                  paper, we present a model for conversational search
                  -- from which we instantiate different observed
                  conversational search strategies, where the agent
                  elicits: (i) Feedback-First, or (ii) Feedback-After.
                  Using 49 TREC WebTrack Topics, we performed an
                  analysis comparing how well these different
                  strategies combine with different mixed initiative
                  approaches: (i) Query Suggestions vs. (ii) Query
                  Clarifications. Our analysis reveals that there is
                  no superior or dominant combination, instead it
                  shows that query clarifications are better when
                  asked first, while query suggestions are better when
                  asked after presenting results. We also show that
                  the best strategy and approach depends on the trade-
                  offs between the relative costs between querying and
                  giving feedback, the performance of the initial
                  query, the number of assessments per query, and the
                  total amount of gain required. While this work
                  highlights the complexities and challenges involved
                  in analyzing CSAs, it provides the foundations for
                  evaluating conversational strategies and
                  conversational search agents in batch/offline
                  settings.}
  NUMPAGES =     {11}
  LOCATION =     {Virtual Event, Queensland, Australia}
  KEYWORDS =     {mixed initiatives, conversational search,
                  evaluation}
  ADDRESS =      {New York, NY, USA}
  PUBLISHER =    {Association for Computing Machinery}
  ISBN =         {9781450384469}
  URL =          {https://doi.org/10.1145/3459637.3482231},
  DOI =          {10.1145/3459637.3482231},
}


@INPROCEEDINGS{10.1145/2505515.2505730,
  AUTHOR =       {Guo, Ting and Chi, Lianhua and Zhu, Xingquan},
  TITLE =        {graph hashing and factorization for fast graph
                  stream classification},
  YEAR =         {2013},
  BOOKTITLE =    {Proceedings of the 22nd ACM International Conference
                  on Information &amp; Knowledge Management},
  SERIES =       {CIKM '13},
  PAGES =        {1607–1612},
  ABSTRACT =     {Graph stream classification concerns building
                  learning models from continuously growing graph
                  data, in which an essential step is to explore
                  subgraph features to represent graphs for effective
                  learning and classification. When representing a
                  graph using subgraph features, all existing methods
                  employ coarse-grained feature representation, which
                  only considers whether or not a subgraph feature
                  appears in the graph. In this paper, we propose a
                  fine-grained graph factorization approach for Fast
                  Graph Stream Classification (FGSC). Our main idea is
                  to find a set of cliques as feature base to
                  represent each graph as a linear combination of the
                  base cliques. To achieve this goal, we decompose
                  each graph into a number of cliques and select
                  discriminative cliques to generate a transfer matrix
                  called Clique Set Matrix (M). By using M as the base
                  for formulating graph factorization, each graph is
                  represented in a vector space with each element
                  denoting the degree of the corresponding subgraph
                  feature related to the graph, so existing supervised
                  learning algorithms can be applied to derive
                  learning models for graph classification.}
  NUMPAGES =     {6}
  LOCATION =     {San Francisco, California, USA}
  KEYWORDS =     {graph stream, matrix factorization, mining
                  algorithm}
  ADDRESS =      {New York, NY, USA}
  PUBLISHER =    {Association for Computing Machinery}
  ISBN =         {9781450322638}
  URL =          {https://doi.org/10.1145/2505515.2505730},
  DOI =          {10.1145/2505515.2505730},
}


@INPROCEEDINGS{10.1145/1935826.1935930,
  AUTHOR =       {Sheldon, Daniel and Shokouhi, Milad and Szummer,
                  Martin and Craswell, Nick},
  TITLE =        {lambdamerge: merging the results of query
                  reformulations},
  YEAR =         {2011},
  BOOKTITLE =    {Proceedings of the Fourth ACM International
                  Conference on Web Search and Data Mining},
  SERIES =       {WSDM '11},
  PAGES =        {795–804},
  ABSTRACT =     {Search engines can automatically reformulate user
                  queries in a variety of ways, often leading to
                  multiple queries that are candidates to replace the
                  original. However, selecting a replacement can be
                  risky: a reformulation may be more effective than
                  the original or significantly worse, depending on
                  the nature of the query, the source of reformulation
                  candidates, and the corpus. In this paper, we
                  explore methods to mitigate this risk by issuing
                  several versions of the query (including the
                  original) and merging their results. We focus on
                  reformulations generated by random walks on the
                  click graph, a method that can produce very good
                  reformulations but is also variable and prone to
                  topic drift. Our primary contribution is λ-Merge, a
                  supervised merging method that is trained to
                  directly optimize a retrieval metric (such as NDCG
                  or MAP) using features that describe both the
                  reformulations and the documents they return. In
                  experiments on Bing data and GOV2, λ-Merge
                  outperforms the original query and several
                  unsupervised merging methods. λ-Merge also
                  outperforms a supervised method to predict and
                  select the best single formulation, and is
                  competitive with an oracle that always selects the
                  best formulation.}
  NUMPAGES =     {10}
  LOCATION =     {Hong Kong, China}
  KEYWORDS =     {learning to rank, query reformulation, lambdamerge,
                  query expansion}
  ADDRESS =      {New York, NY, USA}
  PUBLISHER =    {Association for Computing Machinery}
  ISBN =         {9781450304931}
  URL =          {https://doi.org/10.1145/1935826.1935930},
  DOI =          {10.1145/1935826.1935930},
}


@INPROCEEDINGS{10.1145/1718487.1718493,
  AUTHOR =       {Dang, Van and Croft, Bruce W.},
  TITLE =        {query reformulation using anchor text},
  YEAR =         {2010},
  BOOKTITLE =    {Proceedings of the Third ACM International
                  Conference on Web Search and Data Mining},
  SERIES =       {WSDM '10},
  PAGES =        {41–50},
  ABSTRACT =     {Query reformulation techniques based on query logs
                  have been studied as a method of capturing user
                  intent and improving retrieval effectiveness. The
                  evaluation of these techniques has primarily,
                  however, focused on proprietary query logs and
                  selected samples of queries. In this paper, we
                  suggest that anchor text, which is readily
                  available, can be an effective substitute for a
                  query log and study the effectiveness of a range of
                  query reformulation techniques (including log-based
                  stemming, substitution, and expansion) using
                  standard TREC collections. Our results show that
                  log-based query reformulation techniques are indeed
                  effective with standard collections, but expansion
                  is a much safer form of query modification than word
                  substitution. We also show that using anchor text as
                  a simulated query log is as least as effective as a
                  real log for these techniques.}
  NUMPAGES =     {10}
  LOCATION =     {New York, New York, USA}
  KEYWORDS =     {query substitution, query log, anchor text, query
                  expansion, query reformulation, anchor log}
  ADDRESS =      {New York, NY, USA}
  PUBLISHER =    {Association for Computing Machinery}
  ISBN =         {9781605588896}
  URL =          {https://doi.org/10.1145/1718487.1718493},
  DOI =          {10.1145/1718487.1718493},
}


@INPROCEEDINGS{10.1145/3336191.3371864,
  AUTHOR =       {MacAvaney, Sean},
  TITLE =        {opennir: a complete neural ad-hoc ranking pipeline},
  YEAR =         {2020},
  BOOKTITLE =    {Proceedings of the 13th International Conference on
                  Web Search and Data Mining},
  SERIES =       {WSDM '20},
  PAGES =        {845–848},
  ABSTRACT =     {With the growing popularity of neural approaches for
                  ad-hoc ranking, there is a need for tools that can
                  effectively reproduce prior results and ease
                  continued research by supporting current state-of-
                  the-art approaches. Although several excellent
                  neural ranking tools exist, none offer an easy end-
                  to-end ad-hoc neural raking pipeline. A complete
                  pipeline is particularly important for ad-hoc
                  ranking because there are numerous parameter
                  settings that have a considerable effect on the
                  ultimate performance yet often are under-reported in
                  current work (e.g., initial ranking settings, re-
                  ranking threshold, training sampling strategy,
                  etc.). In this work, I present a complete ad-hoc
                  neural ranking pipeline which addresses these
                  shortcomings: OpenNIR. The pipeline is easy to use
                  (a single command will download required data,
                  train, and evaluate a model), yet highly
                  configurable, allowing for continued work in areas
                  that are understudied. Aside from the core pipeline,
                  the software also includes several bells and
                  whistles that make use of components of the
                  pipeline, such as performance benchmarking and
                  tuning of unsupervised ranker parameters for fair
                  comparisons against traditional baselines. The
                  pipeline and these capabilities are demonstrated.
                  The code is available, and contributions are
                  welcome.}
  NUMPAGES =     {4}
  LOCATION =     {Houston, TX, USA}
  KEYWORDS =     {reproducibility, neural ranking, ad-hoc ranking}
  ADDRESS =      {New York, NY, USA}
  PUBLISHER =    {Association for Computing Machinery}
  ISBN =         {9781450368223}
  URL =          {https://doi.org/10.1145/3336191.3371864},
  DOI =          {10.1145/3336191.3371864},
}


@INPROCEEDINGS{10.1145/3488560.3498534,
  AUTHOR =       {Bondarenko, Alexander and Ajjour, Yamen and Dittmar,
                  Valentin and Homann, Niklas and Braslavski, Pavel
                  and Hagen, Matthias},
  TITLE =        {towards understanding and answering comparative
                  questions},
  YEAR =         {2022},
  BOOKTITLE =    {Proceedings of the Fifteenth ACM International
                  Conference on Web Search and Data Mining},
  SERIES =       {WSDM '22},
  PAGES =        {66–74},
  ABSTRACT =     {In this paper, we analyze comparative questions and
                  answers. At least 3%~of the questions submitted to
                  search engines are comparative; ranging from simple
                  facts like "Did Messi or Ronaldo score more goals in
                  2021?'' to life-changing and probably highly
                  subjective questions like "Is it better to move
                  abroad or stay?''. Ideally, answers to subjective
                  comparative questions would reflect diverse opinions
                  so that the asker can come to a well-informed
                  decision. To better understand the information needs
                  behind comparative questions, we develop approaches
                  to extract the mentioned comparison objects and
                  aspects. As a first step to answer comparative
                  questions, we develop an approach that detects the
                  stances of potential result nuggets (i.e., text
                  passages containing the comparison objects). Our
                  approaches are trained and evaluated on a set of
                  31,000~English questions from existing datasets that
                  we label as comparative or not. In the
                  3,500~comparative questions, we label the comparison
                  objects, aspects, and predicates. For 950~questions,
                  we collect answers from online forums and label the
                  stance towards the comparison objects. In the
                  experiments, our approaches recall~71% of the
                  comparative questions with a perfect precision
                  of~1.0, recall~92% of subjective comparative
                  questions with a precision of~0.98, and identify the
                  comparison objects and aspects with an F1 of~0.93
                  and~0.80, respectively. The stance detector fine-
                  tuned on pairs of objects and answers achieves an
                  accuracy of~0.63.}
  NUMPAGES =     {9}
  LOCATION =     {Virtual Event, AZ, USA}
  KEYWORDS =     {comparative questions, question intent
                  understanding, comparison objects and aspects,
                  answer stance detection}
  ADDRESS =      {New York, NY, USA}
  PUBLISHER =    {Association for Computing Machinery}
  ISBN =         {9781450391320}
  URL =          {https://doi.org/10.1145/3488560.3498534},
  DOI =          {10.1145/3488560.3498534},
}


@INPROCEEDINGS{10.1145/2740908.2742769,
  AUTHOR =       {White, Ryen W. and Richardson, Matthew and Yih, Wen-
                  tau},
  TITLE =        {questions vs queries in informational search tasks},
  YEAR =         {2015},
  BOOKTITLE =    {Proceedings of the 24th International Conference on
                  World Wide Web},
  SERIES =       {WWW '15 Companion},
  PAGES =        {135–136},
  ABSTRACT =     {Search systems traditionally require searchers to
                  formulate information needs as keywords rather than
                  in a more natural form, such as questions. Recent
                  studies have found that Web search engines are
                  observing an increase in the fraction of queries
                  phrased as natural language. As part of building
                  better search engines, it is important to understand
                  the nature and prevalence of these intentions, and
                  the impact of this increase on search engine
                  performance. In this work, we show that while 10.3%
                  of queries issued to a search engine have direct
                  question intent, only 3.2% of them are formulated as
                  natural language questions. We investigate whether
                  search engines perform better when search intent is
                  stated as queries or questions, and we find that
                  they perform equally well to both}
  NUMPAGES =     {2}
  LOCATION =     {Florence, Italy}
  KEYWORDS =     {query formulation, informational search, natural
                  language queries}
  ADDRESS =      {New York, NY, USA}
  PUBLISHER =    {Association for Computing Machinery}
  ISBN =         {9781450334730}
  URL =          {https://doi.org/10.1145/2740908.2742769},
  DOI =          {10.1145/2740908.2742769},
}


@INPROCEEDINGS{10.1145/2872427.2883058,
  AUTHOR =       {Tsur, Gilad and Pinter, Yuval and Szpektor, Idan and
                  Carmel, David},
  TITLE =        {identifying web queries with question intent},
  YEAR =         {2016},
  BOOKTITLE =    {Proceedings of the 25th International Conference on
                  World Wide Web},
  SERIES =       {WWW '16},
  PAGES =        {783–793},
  ABSTRACT =     {Vertical selection is the task of predicting
                  relevant verticals for a Web query so as to enrich
                  the Web search results with complementary vertical
                  results. We investigate a novel variant of this
                  task, where the goal is to detect queries with a
                  question intent. Specifically, we address queries
                  for which the user would like an answer with a human
                  touch. We call these CQA-intent queries, since
                  answers to them are typically found in community
                  question answering (CQA) sites. A typical approach
                  in vertical selection is using a vertical's specific
                  language model of relevant queries and computing the
                  query-likelihood for each vertical as a selective
                  criterion. This works quite well for many domains
                  like Shopping, Local and Travel. Yet, we claim that
                  queries with CQA intent are harder to distinguish by
                  modeling content alone, since they cover many
                  different topics. We propose to also take the
                  structure of queries into consideration, reasoning
                  that queries with question intent have quite a
                  different structure than other queries. We present a
                  supervised classification scheme, random forest over
                  word-clusters for variable length texts, which can
                  model the query structure. Our experiments show that
                  it substantially improves classification performance
                  in the CQA-intent selection task compared to
                  content-oriented based classification, especially as
                  query length grows.}
  NUMPAGES =     {11}
  LOCATION =     {Montr\'{e}al, Qu\'{e}bec, Canada}
  KEYWORDS =     {vertical selection, question intent}
  ADDRESS =      {Republic and Canton of Geneva, CHE}
  PUBLISHER =    {International World Wide Web Conferences Steering
                  Committee}
  ISBN =         {9781450341431}
  URL =          {https://doi.org/10.1145/2872427.2883058},
  DOI =          {10.1145/2872427.2883058},
}


@INPROCEEDINGS{10.1145/3308558.3313411,
  AUTHOR =       {Wang, Hongwei and Zhang, Fuzheng and Zhao, Miao and
                  Li, Wenjie and Xie, Xing and Guo, Minyi},
  TITLE =        {multi-task feature learning for knowledge graph
                  enhanced recommendation},
  YEAR =         {2019},
  BOOKTITLE =    {The World Wide Web Conference},
  SERIES =       {WWW '19},
  PAGES =        {2000–2010},
  ABSTRACT =     {Collaborative filtering often suffers from sparsity
                  and cold start problems in real recommendation
                  scenarios, therefore, researchers and engineers
                  usually use side information to address the issues
                  and improve the performance of recommender systems.
                  In this paper, we consider knowledge graphs as the
                  source of side information. We propose MKR, a Multi-
                  task feature learning approach for Knowledge graph
                  enhanced Recommendation. MKR is a deep end-to-end
                  framework that utilizes knowledge graph embedding
                  task to assist recommendation task. The two tasks
                  are associated by crosscompress units, which
                  automatically share latent features and learn high-
                  order interactions between items in recommender
                  systems and entities in the knowledge graph. We
                  prove that crosscompress units have sufficient
                  capability of polynomial approximation, and show
                  that MKR is a generalized framework over several
                  representative methods of recommender systems and
                  multi-task learning. Through extensive experiments
                  on real-world datasets, we demonstrate that MKR
                  achieves substantial gains in movie, book, music,
                  and news recommendation, over state-of-the-art
                  baselines. MKR is also shown to be able to maintain
                  satisfactory performance even if user-item
                  interactions are sparse.}
  NUMPAGES =     {11}
  LOCATION =     {San Francisco, CA, USA}
  KEYWORDS =     {Multi-task learning, Recommender systems, Knowledge
                  graph}
  ADDRESS =      {New York, NY, USA}
  PUBLISHER =    {Association for Computing Machinery}
  ISBN =         {9781450366748}
  URL =          {https://doi.org/10.1145/3308558.3313411},
  DOI =          {10.1145/3308558.3313411},
}


@ARTICLE{10.1145/3510612,
  AUTHOR =       {Ling, Yanxiang and Cai, Fei and Liu, Jun and Chen,
                  Honghui and de Rijke, Maarten},
  TITLE =        {generating relevant and informative questions for
                  open-domain conversations},
  YEAR =         {2022},
  JOURNAL =      {ACM Trans. Inf. Syst.},
  NOTE =         {Just Accepted}
  PUBLISHER =    {Association for Computing Machinery}
  ABSTRACT =     {Recent research has highlighted the importance of
                  mixed-initiative interactions in conversational
                  search. To enable mixed-initiative interactions,
                  information retrieval systems should be able to ask
                  diverse questions, such as information-seeking,
                  clarification, and open-ended ones. QG of open-
                  domain conversational systems aims at enhancing the
                  interactiveness and persistence of human-machine
                  interactions. The task is challenging because of the
                  sparsity of question generation (QG)-specific data
                  in conversations. Current work is limited to single-
                  turn interaction scenarios. We propose a context-
                  enhanced neural question generation (CNQG) model
                  that leverages the conversational context to predict
                  question content and pattern, then perform question
                  decoding. A hierarchical encoder framework is
                  employed to obtain the discourse-level context
                  representation. Based on this, we propose Review and
                  Transit mechanisms to respectively select contextual
                  keywords and predict new topic words to further
                  construct the question content. Conversational
                  context and the predicted question content are used
                  to produce the question pattern, which in turn
                  guides the question decoding process implemented by
                  a recurrent decoder with a joint attention
                  mechanism. To fully utilize the limited QG-specific
                  data to train our question generator, we perform
                  multi-task learning with three auxiliary training
                  objectives, i.e., question pattern prediction,
                  Review, and Transit mechanisms. The required
                  additional labeled data is obtained in a self-
                  supervised way. We also design a weight decaying
                  strategy to adjust the influences of various
                  auxiliary learning tasks. To the best of our
                  acknowledge, we are the first to extend the
                  application of QG to the multi-turn open-domain
                  conversational scenario. Extensive experimental
                  results demonstrate the effectiveness of our
                  proposal and its main components on generating
                  relevant and informative questions, with robust
                  performance for contexts with various lengths.}
  KEYWORDS =     {context modeling, neural question generation,
                  Conversational search, open-domain conversations}
  MONTH =        {jan}
  ADDRESS =      {New York, NY, USA}
  ISSN =         {1046-8188}
  URL =          {https://doi.org/10.1145/3510612},
  DOI =          {10.1145/3510612},
}


@ARTICLE{10.1145/963770.963773,
  AUTHOR =       {Middleton, Stuart E. and Shadbolt, Nigel R. and De
                  Roure, David C.},
  TITLE =        {ontological user profiling in recommender systems},
  YEAR =         {2004},
  JOURNAL =      {ACM Trans. Inf. Syst.},
  VOLUME =       {22},
  NUMBER =       {1},
  PAGES =        {54–88},
  ABSTRACT =     {We explore a novel ontological approach to user
                  profiling within recommender systems, working on the
                  problem of recommending on-line academic research
                  papers. Our two experimental systems, Quickstep and
                  Foxtrot, create user profiles from unobtrusively
                  monitored behaviour and relevance feedback,
                  representing the profiles in terms of a research
                  paper topic ontology. A novel profile visualization
                  approach is taken to acquire profile feedback.
                  Research papers are classified using ontological
                  classes and collaborative recommendation algorithms
                  used to recommend papers seen by similar people on
                  their current topics of interest. Two small-scale
                  experiments, with 24 subjects over 3 months, and a
                  large-scale experiment, with 260 subjects over an
                  academic year, are conducted to evaluate different
                  aspects of our approach. Ontological inference is
                  shown to improve user profiling, external
                  ontological knowledge used to successfully bootstrap
                  a recommender system and profile visualization
                  employed to improve profiling accuracy. The overall
                  performance of our ontological recommender systems
                  are also presented and favourably compared to other
                  systems in the literature.}
  NUMPAGES =     {35}
  KEYWORDS =     {Agent, machine learning, ontology, recommender
                  systems, user profiling, personalization, user
                  modelling}
  ADDRESS =      {New York, NY, USA}
  ISSN =         {1046-8188}
  PUBLISHER =    {Association for Computing Machinery}
  ISSUE_DATE =   {January 2004}
  MONTH =        {jan}
  URL =          {https://doi.org/10.1145/963770.963773},
  DOI =          {10.1145/963770.963773},
}


@ARTICLE{10.1145/196734.196745,
  AUTHOR =       {Orlikowski, Wanda J. and Gash, Debra C.},
  TITLE =        {technological frames: making sense of information
                  technology in organizations},
  YEAR =         {1994},
  JOURNAL =      {ACM Trans. Inf. Syst.},
  VOLUME =       {12},
  NUMBER =       {2},
  PAGES =        {174–207},
  ABSTRACT =     {In this article, we build on and extend research
                  into the cognitions and values of users and
                  designers by proposing a systematic approach for
                  examining the underlying assumptions, expectations,
                  and knowledge that people have about technology.
                  Such interpretations of technology (which we call
                  technological frames) are central to understanding
                  technological development, use, and change in
                  organizations. We suggest that where the
                  technological frames of key groups in
                  organizations—such as managers, technologists, and
                  users— are significantly different, difficulties and
                  conflict around the development, use, and change of
                  technology may result. We use the findings of an
                  empirical study to illustrate how the nature, value,
                  and use of a groupware technology were interpreted
                  by various organizational stakeholders, resulting in
                  outcomes that deviated from those expected. We argue
                  that technological frames offer an interesting and
                  useful analytic perspective for explaining an
                  anticipating actions and meanings that are not
                  easily obtained with other theoretical lenses.}
  NUMPAGES =     {34}
  KEYWORDS =     {social cognitions, technology use, technological
                  implementation, technological frames, managing
                  expectations}
  ADDRESS =      {New York, NY, USA}
  ISSN =         {1046-8188}
  PUBLISHER =    {Association for Computing Machinery}
  ISSUE_DATE =   {April 1994}
  MONTH =        {apr}
  URL =          {https://doi.org/10.1145/196734.196745},
  DOI =          {10.1145/196734.196745},
}


